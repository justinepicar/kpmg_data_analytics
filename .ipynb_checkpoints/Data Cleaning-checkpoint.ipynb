{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx = pd.ExcelFile('KPMG_VI_New_raw_data_update_final.xlsx')\n",
    "transactions = pd.read_excel(xlsx, 'Transactions', header=[1])\n",
    "new_customers = pd.read_excel(xlsx, 'NewCustomerList', header=[1])\n",
    "demographics = pd.read_excel(xlsx, 'CustomerDemographic', header=[1])\n",
    "address = pd.read_excel(xlsx, 'CustomerAddress', header=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal for this task is to reformat the data based on our quality assessment in the last notebook. We won't impute the missing data for now as we still need to explore the data further for any relationships that we might miss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform product_first_sold_date into a datetime type. First, we need to transform product_first_sold_date to an integer so that we can turn this into a datetime date type. Because we can't ignore null values when transforming an ordinal number to a datetime, we will have to iterate through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.product_first_sold_date = transactions.product_first_sold_date.astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "product_first_sold_date_dt = []\n",
    "\n",
    "for i in transactions.product_first_sold_date.index:\n",
    "    if type(transactions.product_first_sold_date[i]) == np.int64:\n",
    "        new_dt = xlrd.xldate_as_datetime(transactions.product_first_sold_date[i], 0)\n",
    "        product_first_sold_date_dt.append(new_dt)\n",
    "    else:\n",
    "        product_first_sold_date_dt.append(transactions.product_first_sold_date[i])\n",
    "        \n",
    "transactions['product_first_sold_date'] = product_first_sold_date_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if features are unique for id and transaction dates\n",
    "features = transactions.iloc[:,:]\n",
    "\n",
    "for i in features.columns:\n",
    "    print(f'unique {i}: {len(features[i].unique())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There do not seem to be any duplicate transaction ids. All other columns are okay and within range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values and count total values\n",
    "features = transactions.iloc[:,4:]\n",
    "\n",
    "for i in features.columns:\n",
    "    print(f'missing in {i}: {features[i].isna().sum()}')\n",
    "    print(f'{features[i].value_counts()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing features, but these are all formatted correctly with unique variables. Let's check to see how many cancelled orders also have missing values in product line, class, size, cost, and brand. We can see that we have a list price and standard cost, but no column for profits. Let's create this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions['profit'] = round(transactions.list_price - transactions.standard_cost, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ORDER STATUS\n",
    "\n",
    "Change order status to a binary value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.loc[transactions.order_status == 'Approved', 'order_status'] = 1\n",
    "transactions.loc[transactions.order_status == 'Cancelled', 'order_status'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CURRENT CUSTOMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately we can see that default is an irrelevant column and can be dropped, reformat the gender column to a consistent naming convention, and ensure that all date of births are within a reasonable range and makes sense with the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop default\n",
    "demographics.drop('default', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Earliest DOB: {min(demographics.DOB)}')\n",
    "print(f'Latest DOB: {max(demographics.DOB)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the column with Year of Birth, 1843\n",
    "demographics[demographics.DOB == datetime.datetime(1843, 12, 21, 0, 0)]\n",
    "\n",
    "demographics.drop(33, axis=0, inplace=True)\n",
    "demographics.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = demographics.iloc[:,0]\n",
    "print(f'unique customer_id: {len(features.unique())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at range of customer ids\n",
    "demographics.customer_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = demographics.iloc[:,[3, 6, 7, 8, 9, 11]]\n",
    "for i in features.columns:\n",
    "    print(f'{features[i].value_counts()}')\n",
    "    print(f'missing/is null {i}: {features[i].isna().sum()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFORMATTING AND BINNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat gender\n",
    "demographics.loc[demographics.gender == 'F', 'gender'] = 'Female'\n",
    "demographics.loc[demographics.gender == 'Femal', 'gender'] = 'Female'\n",
    "demographics.loc[demographics.gender == 'M', 'gender'] = 'Male'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TENURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simplify tenure, by binning the values. Based on the IQR range, we can break up the data in increments of 5 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=demographics, x='tenure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bins and labels\n",
    "bins = [0, 5, 10, 15, 20, 25]\n",
    "labels = ['0-5 years','6-10 years','11-15 years','16-20 years','21-25 years']\n",
    "#bin data\n",
    "demographics['tenure'] = pd.cut(demographics['tenure'] , bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "sns.countplot(data = demographics, x='tenure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AGE\n",
    "\n",
    "We can also simplify date of births by looking at ages and binning the values into grouped sets of customers. We know that transactions were made in the year 2017 so we can subtract the date of birth from this year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the age of each customer at time of purchase\n",
    "#initialize a column in demographics called 'age'\n",
    "demographics['age'] = np.nan\n",
    "\n",
    "for i in demographics.index:\n",
    "    age = 2017 - demographics.DOB[i].year\n",
    "    demographics.loc[i, 'age'] = age\n",
    "\n",
    "#change dtype to int\n",
    "demographics.age = demographics.age.astype('Int64')\n",
    "\n",
    "# create bins and labels for increments of 10 years\n",
    "bins = [0, 20, 30, 40, 50, 60, 70, 90]\n",
    "labels = ['20 and younger','21-30 years old','31-40 years old','41-50 years old','51-60 years old',\n",
    "          '61-70 years old','70 and up']\n",
    "# bin data\n",
    "demographics['age'] = pd.cut(demographics['age'] , bins=bins, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=demographics, y='age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we grouped tenure and age, we can drop specific Date of Births."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop DOB\n",
    "demographics.drop('DOB', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deceased_indicator and owns_car\n",
    "\n",
    "Let's convert these to binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics.loc[demographics.deceased_indicator == 'Y', 'deceased_indicator'] = 1\n",
    "demographics.loc[demographics.deceased_indicator == 'N', 'deceased_indicator'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics.loc[demographics.owns_car == 'Yes', 'owns_car'] = 1\n",
    "demographics.loc[demographics.owns_car == 'No', 'owns_car'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADDRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "address.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are so many unique postcodes, let's group these based on levels of remoteness. According to the Australian Bureau of Statistics, levels of remoteness are defined as the following:\n",
    "\n",
    "- Major Cities of Australia\n",
    "- Inner Regional Australia\n",
    "- Outer Regional Australia\n",
    "- Remote Australia\n",
    "- Very Remote Australia\n",
    "\n",
    "Remoteness is calculated using the Accessibility and Remoteness Index of Australia (ARIA+). ARIA+ is derived by measuring the road distance from a point to the nearest Urban Centres and Localities in five separate population ranges. For more information, please read about Defining Remote Areas on the ABS website [here](https://www.abs.gov.au/ausstats/abs@.nsf/Latestproducts/1270.0.55.005Main%20Features15July%202016?opendocument&tabname=Summary&prodno=1270.0.55.005&issue=July%202016&num=&view=).\n",
    "\n",
    "We will need to download the 2017 Postcode to 2016 Remoteness Census Area Conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUP BY REMOTENESS INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the ABS file\n",
    "xlsx = pd.ExcelFile('ABS_postcode_2017_ra_2016\\CG_POSTCODE_2017_RA_2016.xlsx')\n",
    "remoteness = pd.read_excel(xlsx, 'Table 3', header=[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remoteness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remoteness.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the NaN value\n",
    "remoteness.drop(0, axis=0, inplace=True)\n",
    "remoteness.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# drop the duplicate postcode, the RA code, and ratio as this is just a duplicate of percentage\n",
    "remoteness.drop(['POSTCODE_2017.1', 'RA_CODE_2016', 'RATIO'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are duplicate postcodes\n",
    "remoteness.POSTCODE_2017.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remoteness[remoteness.POSTCODE_2017 == 4741]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are duplicate postcodes with varying degrees of remoteness due to the large area postcode covers. For simplicity, we will narrow this down to one remoteness area with the highest percentage or ratio that postcode covers. In this case, 90% of postcode 4741 is in Outer Regional Australia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "remoteness = remoteness.sort_values('PERCENTAGE').drop_duplicates('POSTCODE_2017', keep='last').reset_index(drop=True)\n",
    "remoteness[remoteness.POSTCODE_2017 == 4741]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remoteness.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop row 2671 and 2672\n",
    "remoteness.drop([2671, 2672], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "remoteness.rename(columns={'POSTCODE_2017': 'postcode', 'RA_NAME_2016': 'region', 'PERCENTAGE':'percentage'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't need the percentage anymore so we can drop this as well\n",
    "remoteness.drop('percentage', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remoteness.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to merge on the postcodes in address and remoteness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE REMOTE INDEX WITH ADDRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = address.merge(remoteness, how='left', on='postcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = address.iloc[:,-3:]\n",
    "for i in features.columns:\n",
    "    print(f'{features[i].value_counts()}')\n",
    "    print(f'missing/is null {i}: {features[i].isna().sum()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are no missing values. We only need to reformat the state names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address.loc[address.state == 'New South Wales', 'state'] = 'NSW'\n",
    "address.loc[address.state == 'Victoria', 'state'] = 'VIC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=address, y='region', order=address.region.value_counts().index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW CUSTOMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " new_customers.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_customers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right away we can immediately dropped the unnamed columns as this won't be relevant to our analysis. We can generalize our data to age and tenure ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_customers.drop(['Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Earliest DOB: {min(new_customers.DOB)}')\n",
    "print(f'Latest DOB: {max(new_customers.DOB)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = new_customers.iloc[:,[2, 5, 6, 7, 8, 9, 10, 13, 15, 16, 17]]\n",
    "\n",
    "for i in features.columns:\n",
    "    print(f'{features[i].value_counts()}')\n",
    "    print(f'missing/is null {i}: {features[i].isna().sum()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TENURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also bin tenure for new customers with the same intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_customers.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_customers.tenure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bins and labels\n",
    "bins = [0, 5, 10, 15, 20, 25]\n",
    "labels = ['0-5 years','6-10 years','11-15 years','16-20 years','21-25 years']\n",
    "#bin data\n",
    "new_customers['tenure'] = pd.cut(new_customers['tenure'] , bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "sns.countplot(data = new_customers, x='tenure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can bin age once more for the new customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the age of each customer at time of purchase\n",
    "#initialize a column called 'age'\n",
    "new_customers['age'] = np.nan\n",
    "\n",
    "for i in new_customers.index:\n",
    "    age = 2017 - new_customers.DOB[i].year\n",
    "    new_customers.loc[i, 'age'] = age\n",
    "\n",
    "#change dtype to int\n",
    "new_customers.age = new_customers.age.astype('Int64')\n",
    "\n",
    "# create bins and labels for increments of 10 years\n",
    "bins = [0, 20, 30, 40, 50, 60, 70, 90]\n",
    "labels = ['20 and younger','21-30 years old','31-40 years old','41-50 years old','51-60 years old',\n",
    "          '61-70 years old','70 and up']\n",
    "#bin data\n",
    "new_customers['age'] = pd.cut(new_customers['age'] , bins=bins, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we do not have columns for Rank and Value for existing customers, we won't know how to predict based on these features. We can drop these columns from our dataset. However,  we will save this as a new dataframe in case we discover a use for these features as we continue to explore the data. We will also need to merge this with the remoteness index to get an idea of what region these customers are from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_customers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_customers1 = new_customers.drop(['Rank', 'Value', 'DOB'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge remoteness\n",
    "new_customers1 = new_customers1.merge(remoteness, how='left', on='postcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_customers1.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGING THE TABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can explore the data, we need to merge the tables and make sure that there aren't any more reformatting issues, duplicates, or missing values. \n",
    "\n",
    "First, we will join the existing customers to their addresses. We will keep transactions separate for now. We won't be merging the new customers as this will be our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge existing customers to their addresses\n",
    "df = demographics.merge(address, how='outer', on='customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleaned and reformatted our data, we can now explore any trends or patterns. Let's save a copy of these cleaned datasets along with our cleaned new customer list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('KPMG_clean_data.xlsx') as writer:  \n",
    "    df.to_excel(writer, sheet_name='ExistingCustomerList', index=False)\n",
    "    transactions.to_excel(writer, sheet_name='Transactions', index=False)\n",
    "    new_customers1.to_excel(writer, sheet_name='NewCustomerList', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
